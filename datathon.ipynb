{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = ['pandas==1.5.3', \n",
    "            'numpy==1.23.5', \n",
    "            'tqdm==4.64.1', \n",
    "            'requests==2.28.2', \n",
    "            'xgboost==1.7.5', \n",
    "            'scipy==1.10.1', \n",
    "            'lightgbm==3.3.5', \n",
    "            'scikit-learn==1.2.2', \n",
    "            'matplotlib==3.7.1', \n",
    "            'ephem==4.1.4', \n",
    "            'optuna==3.1.1', \n",
    "            'gdown==4.7.1', \n",
    "            'OSMPythonTools==0.3.5']\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(\"\\n\".join(packages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "\n",
    "import xgboost\n",
    "import scipy.stats as stats\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import ephem\n",
    "import optuna\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import gdown\n",
    "\n",
    "from OSMPythonTools.api import Api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TRAIN_NAME = 'train.csv'\n",
    "TRAIN_NAME_NEW = 'new_train.csv'\n",
    "TEST_NAME = 'test.csv'\n",
    "TEST_NAME_NEW = 'new_test.csv'\n",
    "TRAIN_URL = 'https://drive.google.com/file/d/1iT838oJWhZp_ss0ZoazLzy-SbTIHb1Is/view?usp=drive_link'\n",
    "TEST_URL = 'https://drive.google.com/file/d/1MEeMiEyGRcuvzaONA42tAqYCCVEn1dMU/view?usp=drive_link'\n",
    "NEW_TRAIN_URL = 'https://drive.google.com/file/d/1v4R5D7qE_v8Tfz0ClWj2WL0IfgR3cnX9/view?usp=drive_link'\n",
    "NEW_TEST_URL = 'https://drive.google.com/file/d/1y0l0TipyrXq5DU635UUSjGhgyIO-3cwc/view?usp=drive_link'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.csv\n",
    "gdown.download(url=TRAIN_URL, output=TRAIN_NAME, quiet=False, fuzzy=True) \n",
    "# test.csv\n",
    "gdown.download(url=TEST_URL, output=TEST_NAME, quiet=False, fuzzy=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Uncomment & run the code bellow if you want to skip the API extraction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # new_train.csv\n",
    "# gdown.download(url=NEW_TRAIN_URL, output=TRAIN_NAME_NEW, quiet=False, fuzzy=True) \n",
    "# # new_test.csv\n",
    "# gdown.download(url=NEW_TEST_URL, output=TEST_NAME_NEW, quiet=False, fuzzy=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_NAME)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(TEST_NAME_NEW)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract API Data from id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_jalan_train = df_train['id_jalan'].unique()\n",
    "unique_jalan_test = df_test['id_jalan'].unique()\n",
    "unique_jalan = np.concatenate((unique_jalan_train, unique_jalan_test), axis=0)\n",
    "unique_jalan = np.unique(unique_jalan, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mulai_train = df_train['id_titik_mulai'].unique()\n",
    "unique_mulai_test = df_test['id_titik_mulai'].unique()\n",
    "unique_akhir_train = df_train['id_titik_akhir'].unique()\n",
    "unique_akhir_test = df_test['id_titik_akhir'].unique()\n",
    "unique_mulai = np.concatenate((unique_mulai_train, unique_mulai_test), axis=0)\n",
    "unique_mulai = np.unique(unique_mulai, axis=0)\n",
    "unique_akhir = np.concatenate((unique_akhir_train, unique_akhir_test), axis=0)\n",
    "unique_akhir = np.unique(unique_akhir, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_mulai_akhir_train = np.unique(df_train[['id_titik_mulai', 'id_titik_akhir']].values, axis=0)\n",
    "unique_mulai_akhir_test = np.unique(df_test[['id_titik_mulai', 'id_titik_akhir']].values, axis=0)\n",
    "unique_mulai_akhir = np.concatenate((unique_mulai_akhir_train, unique_mulai_akhir_test), axis=0)\n",
    "unique_mulai_akhir = np.unique(unique_mulai_akhir, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OSM API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing result\n",
    "api = Api()\n",
    "way = api.query('way/66924592')\n",
    "print(way.tags())\n",
    "node = api.query('node/21390008') # idx=0, id_titik mulai\n",
    "print(node.lat())\n",
    "print(node.lon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing result\n",
    "node = api.query('node/21390008') # idx=0, id_titik mulai\n",
    "print(node.lat())\n",
    "print(node.lon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_jalan = {'id_jalan': unique_jalan,\n",
    "                'api_result': []}\n",
    "api = Api()\n",
    "# new_col_jalan = ['lanes', 'lanes:forward', 'lit', 'maxspeed']\n",
    "new_col_jalan = ['cycleway', 'highway','lanes', 'lanes:forward', 'lit', 'maxspeed', 'name', 'operator', 'ref', 'sidewalk', 'surface', 'turn:lanes:forward']\n",
    "for id_jalan in unique_jalan:\n",
    "    way = api.query(f'way/{id_jalan}')\n",
    "    api_result = {} \n",
    "    for col in new_col_jalan:\n",
    "        api_result[col] = way.tag(col)\n",
    "    result_jalan['api_result'].append(api_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_extract(node):\n",
    "    url = f'https://www.openstreetmap.org/api/0.6/node/{node}/history'\n",
    "    r = requests.get(url)\n",
    "    root = ET.fromstring(r.text)\n",
    "\n",
    "    # Find all nodes with the specified id\n",
    "    nodes = root.findall(f'.//node[@id=\"{node}\"]')\n",
    "\n",
    "    # Initialize variables to hold latitude and longitude\n",
    "    latest_lat = None\n",
    "    latest_lon = None\n",
    "\n",
    "    # Iterate through the nodes in reverse order (latest version first)\n",
    "    for node in reversed(nodes):\n",
    "        lat = node.get('lat')\n",
    "        lon = node.get('lon')\n",
    "        \n",
    "        if lat and lon:\n",
    "            latest_lat = lat\n",
    "            latest_lon = lon\n",
    "            break  # Stop when the first valid lat and lon are found\n",
    "\n",
    "    # If no valid lat and lon are found in the latest version, use the previous version's data\n",
    "    if latest_lat is None or latest_lon is None:\n",
    "        for node in reversed(nodes):\n",
    "            lat = node.get('lat')\n",
    "            lon = node.get('lon')\n",
    "            if lat and lon:\n",
    "                latest_lat = lat\n",
    "                latest_lon = lon\n",
    "                break  # Stop when the first valid lat and lon are found\n",
    "    return latest_lat, latest_lon\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mulai = {'id_titik_mulai': unique_mulai,\n",
    "                'api_result': []}\n",
    "api = Api()\n",
    "new_col_jalan = ['lat', 'lon']\n",
    "for id_titik in unique_mulai:\n",
    "    try:\n",
    "        node = api.query(f'node/{id_titik}')\n",
    "    except:\n",
    "        lat, lon = manual_extract(id_titik)\n",
    "    \n",
    "    api_result = {'lat': node.lat(),\n",
    "                  'lon': node.lon()} \n",
    "    result_mulai['api_result'].append(api_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_akhir = {'id_titik_akhir': unique_akhir,\n",
    "                'api_result': []}\n",
    "api = Api()\n",
    "new_col_jalan = ['lat', 'lon']\n",
    "for id_titik in unique_akhir:\n",
    "    try:\n",
    "        node = api.query(f'node/{id_titik}')\n",
    "    except:\n",
    "        lat, lon = manual_extract(id_titik)\n",
    "    \n",
    "    api_result = {'lat': node.lat(),\n",
    "                  'lon': node.lon()} \n",
    "    result_akhir['api_result'].append(api_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MapQuest API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(start_lat, start_lon, end_lat, end_lon):\n",
    "    key = 'FVgErOkhYoJmjdsUrldVi9nkrCrGKuWm' # API KEY from MapQuest\n",
    "    url = f'https://www.mapquestapi.com/directions/v2/route?key={key}&from={start_lat},{start_lon}&to={end_lat},{end_lon}'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    dist = data['route']['distance']\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_distance = {'id_titik_mulai_akhir': unique_mulai_akhir,\n",
    "                'api_result': []}\n",
    "api = Api()\n",
    "new_col_jalan = ['lat', 'lon']\n",
    "for id_titik in unique_mulai_akhir:\n",
    "    idx_mulai = np.where(result_mulai['id_titik_mulai'] == id_titik[0])[0].sum()\n",
    "    idx_akhir = np.where(result_akhir['id_titik_akhir'] == id_titik[1])[0].sum()\n",
    "    dist = distance(result_mulai['api_result'][idx_mulai]['lat'], result_mulai['api_result'][idx_mulai]['lon'], result_akhir['api_result'][idx_akhir]['lat'], result_akhir['api_result'][idx_akhir]['lon'])\n",
    "    api_result = {'distance': dist} \n",
    "    result_distance['api_result'].append(api_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_idx_dist(id_titik, unique_val):\n",
    "    for idx, val in enumerate(unique_val):\n",
    "        if np.array_equal(val, id_titik):\n",
    "            return idx\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_data(df, filename):\n",
    "    # new_col = ['lanes', 'lanes:forward', 'lit', 'maxspeed', 'mulai_lat', 'mulai_lon', 'akhir_lat', 'akhir_lon', 'distance']\n",
    "    new_col = ['cycleway', 'highway','lanes', 'lanes:forward', 'lit', 'maxspeed', 'name', 'operator', 'ref', 'sidewalk', 'surface', 'turn:lanes:forward', 'mulai_lat', 'mulai_lon', 'akhir_lat', 'akhir_lon', 'distance']\n",
    "    additional_data = {'cycleway':[], 'highway':[],'lanes':[], 'lanes:forward':[], 'lit':[], 'maxspeed':[], 'name':[], 'operator':[], 'ref':[], 'sidewalk':[], 'surface':[], 'turn:lanes:forward':[], 'mulai_lat':[], 'mulai_lon':[], 'akhir_lat':[], 'akhir_lon':[], 'distance':[]}\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        id_jalan = row['id_jalan']\n",
    "        idx_jalan = np.where(result_jalan['id_jalan'] == id_jalan)[0].sum()\n",
    "\n",
    "        cycleway, highway, name, operator = (result_jalan['api_result'][idx_jalan]['cycleway'], result_jalan['api_result'][idx_jalan]['highway'], result_jalan['api_result'][idx_jalan]['name'], result_jalan['api_result'][idx_jalan]['operator'])\n",
    "        lanes, lanesforward, lit, maxspeed = (result_jalan['api_result'][idx_jalan]['lanes'], result_jalan['api_result'][idx_jalan]['lanes:forward'], result_jalan['api_result'][idx_jalan]['lit'], result_jalan['api_result'][idx_jalan]['maxspeed'])\n",
    "        ref, sidewalk, surface, turnlanesforward = (result_jalan['api_result'][idx_jalan]['ref'], result_jalan['api_result'][idx_jalan]['sidewalk'], result_jalan['api_result'][idx_jalan]['surface'], result_jalan['api_result'][idx_jalan]['turn:lanes:forward'])\n",
    "\n",
    "        id_titik_mulai = row['id_titik_mulai']\n",
    "        id_titik_akhir = row['id_titik_akhir']\n",
    "\n",
    "        idx_mulai = np.where(result_mulai['id_titik_mulai'] == id_titik_mulai)[0].sum()\n",
    "        idx_akhir = np.where(result_akhir['id_titik_akhir'] == id_titik_akhir)[0].sum()\n",
    "\n",
    "        mulai_lat, mulai_lon = (result_mulai['api_result'][idx_mulai]['lat'], result_mulai['api_result'][idx_mulai]['lon'])\n",
    "        akhir_lat, akhir_lon = (result_akhir['api_result'][idx_akhir]['lat'], result_akhir['api_result'][idx_akhir]['lon'])\n",
    "\n",
    "        id_distance = np.array([row['id_titik_mulai'], row['id_titik_akhir']])\n",
    "        idx_distance = find_idx_dist(id_distance, unique_mulai_akhir)\n",
    "        distance = result_distance['api_result'][idx_distance]['distance']\n",
    "\n",
    "        additional_data['cycleway'].append(cycleway)\n",
    "        additional_data['highway'].append(highway)\n",
    "        additional_data['name'].append(name)\n",
    "        additional_data['operator'].append(operator)\n",
    "        additional_data['lanes'].append(lanes)\n",
    "        additional_data['lanes:forward'].append(lanesforward)\n",
    "        additional_data['lit'].append(lit)\n",
    "        additional_data['maxspeed'].append(maxspeed)\n",
    "        additional_data['ref'].append(ref)\n",
    "        additional_data['sidewalk'].append(sidewalk)\n",
    "        additional_data['surface'].append(surface)\n",
    "        additional_data['turn:lanes:forward'].append(turnlanesforward)\n",
    "        \n",
    "        additional_data['mulai_lat'].append(mulai_lat)\n",
    "        additional_data['mulai_lon'].append(mulai_lon)\n",
    "        additional_data['akhir_lat'].append(akhir_lat)\n",
    "        additional_data['akhir_lon'].append(akhir_lon)\n",
    "        additional_data['distance'].append(distance)\n",
    "    for col in new_col:\n",
    "        df[col] = additional_data[col]\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_data(df_train, TRAIN_NAME_NEW)\n",
    "add_data(df_test, TEST_NAME_NEW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_NAME_NEW)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepraring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    df = df.drop(['lanes', 'lanes:forward', 'lit'], axis=1)\n",
    "\n",
    "    unique_maxspeed = df['maxspeed'].unique()\n",
    "    max_speed = []\n",
    "    for data in tqdm(df['maxspeed'].to_numpy(), desc='Convert to kph'):\n",
    "        if data == '30 mph':\n",
    "            max_speed.append(48.2803)\n",
    "        elif data == '20 mph':\n",
    "            max_speed.append(32.1869)\n",
    "\n",
    "    df['maxspeed'] = max_speed\n",
    "    \n",
    "    is_weekend = []\n",
    "    is_night = []\n",
    "    is_rush_hour = []\n",
    "    date = []\n",
    "    hour = []\n",
    "\n",
    "    uk_observer = ephem.Observer()\n",
    "    uk_observer.lat = '51.5074'  # Latitude of London\n",
    "    uk_observer.lon = '-0.1278'  # Longitude of London\n",
    "\n",
    "\n",
    "    for data in tqdm(df['waktu_setempat'].to_numpy(), desc='time categorization'):\n",
    "        datetime_obj = datetime.strptime(data, '%Y-%m-%d %H:%M:%S%z')\n",
    "        day_of_week = datetime_obj.weekday()\n",
    "        if day_of_week >= 5:\n",
    "            is_weekend.append(1)\n",
    "        else:\n",
    "            is_weekend.append(0)\n",
    "        date_component = datetime_obj.strftime('%Y-%m-%d')\n",
    "        hour_component = datetime_obj.strftime('%H')\n",
    "        date.append(date_component)\n",
    "        hour.append(int(hour_component))\n",
    "        # Set the observer's date and time to the input UTC time\n",
    "        uk_observer.date = datetime_obj\n",
    "\n",
    "        # Calculate sunrise and sunset times\n",
    "        sunrise = uk_observer.previous_rising(ephem.Sun())\n",
    "        sunset = uk_observer.next_setting(ephem.Sun())\n",
    "        if sunrise < uk_observer.date < sunset:\n",
    "            is_night.append(0)\n",
    "        else:\n",
    "            is_night.append(1)\n",
    "        \n",
    "        # Define the time ranges\n",
    "        morning_rush_hour_start = datetime.strptime('10:00:00', '%H:%M:%S').time()\n",
    "        morning_rush_hour_end = datetime.strptime('16:00:00', '%H:%M:%S').time()\n",
    "\n",
    "        night_rush_hour_start = datetime.strptime('20:00:00', '%H:%M:%S').time()\n",
    "        night_rush_hour_end = datetime.strptime('23:59:59', '%H:%M:%S').time()\n",
    "\n",
    "        night_rush_hour_start_2 = datetime.strptime('00:00:00', '%H:%M:%S').time()\n",
    "        night_rush_hour_end_2 = datetime.strptime('06:00:00', '%H:%M:%S').time()\n",
    "\n",
    "        # Extract the time component from the input datetime object\n",
    "        input_time = datetime_obj.time()\n",
    "\n",
    "        # Check if the time falls within the desired ranges\n",
    "        if morning_rush_hour_start <= input_time <= morning_rush_hour_end or (night_rush_hour_start <= input_time <= night_rush_hour_end) or (night_rush_hour_start_2 <= input_time <= night_rush_hour_end_2):\n",
    "            is_rush_hour.append(1)\n",
    "        else:\n",
    "            is_rush_hour.append(0)\n",
    "\n",
    "    df['is_weekend'] = is_weekend\n",
    "    df['hour'] = hour\n",
    "    df['date'] = date\n",
    "    df['is_night'] = is_night\n",
    "    df['is_rush_hour'] = is_rush_hour\n",
    "    df = df.drop(['waktu_setempat'], axis=1)\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prepare_data(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_used = ['maxspeed','mulai_lat','mulai_lon','akhir_lat','akhir_lon','distance','is_weekend','hour','is_night','is_rush_hour']\n",
    "X = df[col_used]\n",
    "y = df['rerata_kecepatan'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to return the SMAPE value\n",
    "def smape(actual, predicted) -> float:\n",
    "  \n",
    "    # Convert actual and predicted to numpy\n",
    "    # array data type if not already\n",
    "    if not all([isinstance(actual, np.ndarray), \n",
    "                isinstance(predicted, np.ndarray)]):\n",
    "        actual, predicted = np.array(actual),\n",
    "        np.array(predicted)\n",
    "  \n",
    "    return round(\n",
    "        np.mean(\n",
    "            np.abs(predicted - actual) / \n",
    "            ((np.abs(predicted) + np.abs(actual))/2)\n",
    "        )*100, 6\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval(pred, true):\n",
    "    residuals = true - pred\n",
    "\n",
    "    # Create a 2x3 grid of subplots\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Scatter plot of real vs. predicted values\n",
    "    axes[0, 0].scatter(true, pred)\n",
    "    axes[0, 0].set_xlabel('Real Values')\n",
    "    axes[0, 0].set_ylabel('Predicted Values')\n",
    "    axes[0, 0].set_title('Scatter Plot')\n",
    "\n",
    "    # Residual plot\n",
    "    axes[0, 1].scatter(pred, residuals)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "\n",
    "    # Histogram of residuals\n",
    "    axes[0, 2].hist(residuals, bins=20)\n",
    "    axes[0, 2].set_xlabel('Residuals')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].set_title('Histogram of Residuals')\n",
    "\n",
    "    # QQ plot\n",
    "    stats.probplot(residuals.flatten(), plot=axes[1, 0])\n",
    "    axes[1, 0].set_title('QQ Plot')\n",
    "\n",
    "    # Regression line plot\n",
    "    axes[1, 1].scatter(true, pred)\n",
    "    axes[1, 1].plot(true, true, color='r', linestyle='--')\n",
    "    axes[1, 1].set_xlabel('Real Values')\n",
    "    axes[1, 1].set_ylabel('Predicted Values')\n",
    "    axes[1, 1].set_title('Regression Line Plot')\n",
    "\n",
    "    # R-squared and MSE values\n",
    "    r2 = r2_score(true, pred)\n",
    "    mse = mean_squared_error(true, pred)\n",
    "    axes[1, 2].bar(['R-squared', 'MSE'], [r2, mse])\n",
    "    axes[1, 2].set_title('R-squared and MSE')\n",
    "\n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBRegressor(n_jobs=-1, random_state=42)\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)\n",
    "print(f'test_data: {smape(y_test, pred)}')\n",
    "plot_eval(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optuna: Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Usually it takes more than one hour for each model. So its better to use the already existing best parameters.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Regressor Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    \n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 5000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
    "    gamma = trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 7, step=2)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1.0, step=0.1)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1.0, step=0.1)\n",
    "    seed = trial.suggest_int(\"random_state\", 20, 50, step=2)\n",
    "    \n",
    "    \n",
    "    model = xgboost.XGBRegressor(n_estimators=n_estimators,\n",
    "                                max_depth=max_depth,\n",
    "                                learning_rate=learning_rate,\n",
    "                                gamma=gamma,\n",
    "                                min_child_weight=min_child_weight,\n",
    "                                colsample_bytree=colsample_bytree,\n",
    "                                subsample=subsample,\n",
    "                                reg_alpha=reg_alpha,\n",
    "                                reg_lambda=reg_lambda,\n",
    "                                n_jobs=-1, metric=mean_squared_error,\n",
    "                                eval_metric=mean_squared_error,\n",
    "                                random_state=seed\n",
    "                                )\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_hat = model.predict(x_test)\n",
    "    \n",
    "    return mean_squared_error(y_test, y_hat, squared=True)\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bellow are the best parameters for XGBoost.`\n",
    "\n",
    "param_1 = {'n_estimators': 2552, 'max_depth': 9, 'learning_rate': 0.010419707354527082, 'gamma': 0.30000000000000004, 'min_child_weight': 5, 'subsample': 0.8, 'colsample_bytree': 0.9, 'reg_alpha': 0.30000000000000004, 'reg_lambda': 0.5, 'random_state': 28}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LGBM Regressor Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    \n",
    "    subsample_for_bin = trial.suggest_int(\"subsample_for_bin\", 100000, 300000)\n",
    "    n_estimators = trial.suggest_int(\"n_estimators\", 1000, 5000)\n",
    "    max_depth = trial.suggest_int(\"max_depth\", 1, 10)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 1e-1, log=True)\n",
    "    num_leaves = trial.suggest_int(\"num_leaves\", 10, 50)\n",
    "    min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 7, step=2)\n",
    "    subsample = trial.suggest_float(\"subsample\", 0.5, 1.0, step=0.1)\n",
    "    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.5, 1.0, step=0.1)\n",
    "    reg_alpha = trial.suggest_float(\"reg_alpha\", 0., 1.0, step=0.1)\n",
    "    reg_lambda = trial.suggest_float(\"reg_lambda\", 0., 1.0, step=0.1)\n",
    "    seed = trial.suggest_int(\"random_state\", 20, 50, step=2)\n",
    "    min_child_samples = trial.suggest_int(\"min_child_samples\", 10, 50)\n",
    "    \n",
    "    model = lgb.LGBMRegressor(num_leaves=num_leaves, max_depth=max_depth, learning_rate=learning_rate, \n",
    "                              n_estimators=n_estimators, subsample_for_bin=subsample_for_bin, \n",
    "                              min_child_weight=min_child_weight, min_child_samples=min_child_samples, subsample=subsample, \n",
    "                              colsample_bytree=colsample_bytree, reg_alpha=reg_alpha, \n",
    "                              reg_lambda=reg_lambda, random_state=seed, n_jobs=-1,)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    y_hat = model.predict(x_test)\n",
    "    \n",
    "    return mean_squared_error(y_test, y_hat, squared=True)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bellow are the best parameters for LGBM.`\n",
    "\n",
    "param_1 = {'subsample_for_bin': 131673, 'n_estimators': 4510, 'max_depth': 6, 'learning_rate': 0.054834260577325884, 'num_leaves': 47, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.30000000000000004, 'random_state': 34, 'min_child_samples': 16}\n",
    "\n",
    "param_2 = {'subsample_for_bin': 155546, 'n_estimators': 3768, 'max_depth': 7, 'learning_rate': 0.04067420520019853, 'num_leaves': 50, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 0.30000000000000004, 'random_state': 34, 'min_child_samples': 16}\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_1(no):\n",
    "  param_1 = {'n_estimators': 2552, \n",
    "             'max_depth': 9, \n",
    "             'learning_rate': 0.010419707354527082, \n",
    "             'gamma': 0.30000000000000004, \n",
    "             'min_child_weight': 5, \n",
    "             'subsample': 0.8, \n",
    "             'colsample_bytree': 0.9, \n",
    "             'reg_alpha': 0.30000000000000004, \n",
    "             'reg_lambda': 0.5, \n",
    "             'random_state': 28}\n",
    "  \n",
    "  params = [param_1]\n",
    "  xgb = xgboost.XGBRegressor(**params[no])\n",
    "  return xgb\n",
    "\n",
    "def lgbm_1(no):\n",
    "  param_1 = {'subsample_for_bin': 131673, 'n_estimators': 4510, 'max_depth': 6, 'learning_rate': 0.054834260577325884, 'num_leaves': 47, 'min_child_weight': 1, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.7000000000000001, 'reg_lambda': 0.30000000000000004, 'random_state': 34, 'min_child_samples': 16}\n",
    "\n",
    "  param_2 = {'subsample_for_bin': 155546, 'n_estimators': 3768, 'max_depth': 7, 'learning_rate': 0.04067420520019853, 'num_leaves': 50, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 1.0, 'reg_alpha': 0.5, 'reg_lambda': 0.30000000000000004, 'random_state': 34, 'min_child_samples': 16}\n",
    "  \n",
    "  params = [param_1, param_2]\n",
    "  lgbm = lgb.LGBMRegressor(**params[no])\n",
    "  return lgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators1 = [('xgb_1{}'.format(i), xgb_1(i)) for i in range(1)]\n",
    "estimators2 = [('lgbm_1{}'.format(j), lgbm_1(j))  for j in range(2)]\n",
    "estimators = estimators1+estimators2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for est in estimators:\n",
    "    name, reg = est\n",
    "    print(f'Evaluating {name} model')\n",
    "    reg.fit(x_train, y_train)\n",
    "    pred = reg.predict(x_test)\n",
    "    print(f'val_data: {smape(y_test, pred)}')\n",
    "    pred = reg.predict(x_train)\n",
    "    print(f'train_data: {smape(y_train, pred)}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StackingRegressor(estimators=estimators, \n",
    "                          cv=3, \n",
    "                          final_estimator=LinearRegression(n_jobs=-1), \n",
    "                          n_jobs = -1, \n",
    "                          verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)\n",
    "print(f'val_data: {smape(y_test, pred)}')\n",
    "plot_eval(pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_train)\n",
    "print(f'train_data: {smape(y_train, pred)}')\n",
    "plot_eval(pred, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying on test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_test = pd.read_csv(TEST_NAME_NEW)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = prepare_data(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = df_test[col_used]\n",
    "\n",
    "\n",
    "pred_test = model.predict(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['rerata_kecepatan'] = pred_test\n",
    "subm = df_test[['id', 'rerata_kecepatan']]\n",
    "subm.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
